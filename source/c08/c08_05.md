# 8.5 OpenStack 源码剖析

## 虚拟机是如何创建出来的？

## rpc 是如何调用的？

## 创建快照代码解读？



## 虚拟机状态

vm_state 描述虚拟机当前的稳定状态，其值可以在 `nova/compute/vm_states.py`看到

```
ACTIVE
BUILDING
PAUSED
SUSPENDED
STOPPED
RESCUED
RESIZED
SOFT_DELETED
DELETED
ERROR
SHELVED
```

power_state 描述的是从hypervisor传过来的状态，其值可在`nova/compute/power_state.py`

```
NOSTATE
RUNNING
PAUSED
SHUTDOWN
CRASHED
SUSPENDED
```

task_state 描述的是中间任务状态，

```
spawning
networking
scheduling
block_device_mapping
```

在创建虚拟机时，会有几次会产生虚拟机状态（vm_state和task_state）的上报（到ceilomet er）。

nova 提供了一个配置项：notify_on_state_change，本意是想，如果配置`vm_state`就只在vm_state

第一次，在`manager.py:2050`的函数 `_do_build_and_run_instance`里，看instance.save()大



## 快照镜像

入口函数在：`nova/virt/libvirt/driver.py:snapshot()`

会先获取imagebackend的类型，然后找到对应的backend

```python
disk_path, source_format = libvirt_utils.find_disk(virt_dom)
source_type = libvirt_utils.get_disk_type_from_path(disk_path)
...
snapshot_backend = self.image_backend.snapshot(instance,
                                               disk_path,
                                               image_type=source_type)
```

接下来，会调用对应的imagebackend的`snapshot_extract` 方法。

![](http://image.python-online.cn/FhRPy4B1xEI9SfoD2RcunJl15ZE3)

`snapshot_extract` 方法最终会调用`nova/virt/images.py:_convert_image()` ，可以看出其实底层调用的是 `qemu-img` 提供的`convert` 接口。

![](http://image.python-online.cn/FuyMWZS6HF4g3rPwTlLcereZxg4L)

如果是qcow2的backend，不是调用这边，而是调用 `nova/virt/libvirt/utils.py:extract_snapshot()`

![1551944122412](C:\Users\wangbm\AppData\Roaming\Typora\typora-user-images\1551944122412.png)

如果要查询镜像压缩的时间，可以在compute上执行这个命令

```
grep -E "Start executing commands|End executing commands" /var/log/nova/nova-compute.log
```

以上，就是整个镜像创建的过程。

独立磁盘模式的暂时不支持，原因分析如下。

在`libvirt_utils.get_disk_type_from_path` 里没有相应的修改，导致返回的是lvm。

![](http://image.python-online.cn/FnJA8RNIvJN2lAEXbKtJDpOLg1vg)

后面的imagebackend也相应的变成 lvm的

![](http://image.python-online.cn/FnGyI8jCQFLCGi0pGVmI3SV6pDrv)

然后会进入 lvm这个backend的init函数。由于`path` 是`/dev/sdb` 并不是一个lv，所以这边会报错。

![1551940635806](C:\Users\wangbm\AppData\Roaming\Typora\typora-user-images\1551940635806.png)

下次修改方法：一个是最开始获取`source_type`时判断为isolate，一个是后面 `isolate`的`extract_snapshot` 也要和lvm一样实现一下。



## 资源上报

compute的资源上报，是在 `nova/compute/resource_tracker.py:_init_compute_node` 里。

从宿主机上获取数据：`update_available_resource` 函数下的 `resources = self.driver.get_available_resource(self.nodename)` 其调用的函数是`virt/libvirt/driver.py` 里的 `get_available_resource` 函数

![](http://image.python-online.cn/FrbE6oEZ3vtTWwDfMNQ16MGi6SWr)


从数据库获取旧数据 `self.compute_node = self._get_compute_node(context)` 

## cloud-init

cloud-init 是 linux 的一个工具，当系统启动时，cloud-init 可从 nova metadata 服务或者 config drive 中获取 metadata，完成一些虚拟机的初始化工作，这些工作有可能是每台虚拟机例行的动作，如配置ip，也有可能是定制化动作，如注入密码等。

为了实现 instance 定制工作，cloud-init 会按 4 个阶段执行任务：

1. local
2. init
3. config
4. final

一开始，我很疑惑这些任务是如何执行的，cloud-init 既没有服务，也没有加入rc.local 中。直到某天我意外地执行了`chkconfig --list`， 原来这些脚本也可以以这样的方式开机自启。

![](http://image.python-online.cn/FvK6li1P4mPIZv9RsZ1J4kV6cgFF)

当然这几个阶段你也可以通过执行命令来手动执行

1. cloud-init init --local 或者 cloud-init init -l
2. cloud-init init  或者 cloud-init modules -m init 
3. cloud-init modules -m config 
4. cloud-init modules -m final

除了这几个命令之外，还有几个

- cloud-init query -n [name]
- cloud-init single -n [name] --frequency

以上，这些命令都是看源码我才得知的，以下是对应阶段处理的函数

- `init`：main_init
- `modules`：main_modules
- `query`：main_query
- `single`：main_single

在 init 阶段，有一个非常重要的步骤是配置网络，接下来就来源码解读一下这个流程在代码里是如何进行的。

首先当然是入口文件，如果是 比较早期的版本，入口文件是在 `/usr/bin/cloud-init` 如果是较新的版本，入口文件是在 `cloudinit/cmd/main.py` ，函数调用如下图

![](http://image.python-online.cn/FntUO_tNeRLN21uu_PowFhZ_wyBu)

其中 `on_first_boot` 是在cloudinit 判断为新虚拟机时才会执行。也就是说，这个部署网络的步骤在一个虚拟机生命周期里，只会执行一次，如果要二次执行，可执行 `rm -rf /var/lib/cloud` ，将缓存数据删除。

而在 `get_data` 里，会先创建一个临时目录，将 ` /dev/sr0` 挂载过来

![](http://image.python-online.cn/FpqcyL4hWwpaAGzsdreQwXvH4Rx8)

然后从 `ConfigDrive` 里读取数据，而后将读取的数据存入 `/var/lib/cloud/instance/obj.pkl`， 而后续执行都将从这里反序列化，提高速度。

而如果从 `ConfigDrive` 获取不到数据，则会跳过网卡配置，将扫描出来的第一块网卡配置成 dhcp 模式。。这是非常关键的一步，只有当网卡正确配置后，才能获取到 metadata。

如果是按照旧虚拟机创建新的快照镜像，然后使用这个镜像创建新的虚拟机，有可能会在同一块网卡上出现新旧两个ip，这是因为虚拟机在启动过程中，会先读取原网络配置配置ip，然后才会运行 cloud-init 进行新ip的配置，而新ip的配置是使用  `ifup`  这个命令![](http://image.python-online.cn/Fp1TeHSiIMIQoZygbW9VSfAagB_d)

使用这种方式并不会将第一次配置的旧ip给清除掉。![](http://image.python-online.cn/Fh-5SQ8qYjhJEKovI6LmIpabSy2c)

这个问题，目前我只在CentOS6 中遇到过。通过先 `ifdown` 再 `ifup` 就可以解决这个问题。

```python
    def _bring_up_interface(self, device_name):
        cmd_down = ['ifdown', device_name]
        cmd_up = ['ifup', device_name]

        LOG.debug("Attempting to run bring up interface %s using command %s",
                   device_name, cmd_up)
        try:
            (_out, err) = util.subp(cmd_down)
            if len(err):
                LOG.warn("Running %s resulted in stderr output: %s", cmd_down, err)
            (_out, err) = util.subp(cmd_up)
            if len(err):
                LOG.warn("Running %s resulted in stderr output: %s", cmd_up, err)
            return True
        except util.ProcessExecutionError:
            util.logexc(LOG, "Running interface command %s failed", cmd_up)
            return False
```



