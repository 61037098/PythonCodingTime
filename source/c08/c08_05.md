# 8.5 OpenStack 源码剖析与改造

## 8.5.1 虚拟机是如何创建出来的？



生成xml，准备网络（plug_vif）创建domain 是在 `_create_domain_and_network` 这个函数中

![](http://image.python-online.cn/20190526144846.png)

这个函数，只有在 `spawn` 、`_hard_reboot` 和 `finish_migration` 才会执行。



## 8.5.2 rpc 是如何调用的？

OpenStack的通信方式有两种，一种是基于HTTP协议的RESTFul API方式，另一种则是RPC调用。两种通信方式的应用场景有所不同，在OpenStack中，前者主要用于各组件之间的通信（如nova与glance的通信），而后者则用于同一组件中各个不同模块之间的通信（如nova组件中nova-compute与nova-scheduler的通信）。
关于OpenStack中基于RESTFul API的通信方式主要是应用了WSGI，我会在以后的博文中详细讨论。这里主要还是分享一下关于RPC调用的一些理解。

首先，什么是RPC呢？百度百科给出的解释是这样的：“RPC（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议”。这个概念听起来还是比较抽象，下面我会结合OpenStack中RPC调用的具体应用来具体分析一下这个协议。

其次，为什么要采用RPC呢？单纯的依靠RESTFul API不可以吗？其实原因有下面这几个：

1. 由于RESTFul API是基于HTTP协议的，因此客户端与服务端之间所能传输的消息仅限于文本
2. RESTFul API客户端与服务端之间采用的是同步机制，当发送HTTP请求时，客户端需要等待服务端的响应。当然对于这一点是可以通过一些技术来实现异步的机制的
3. 采用RESTFul API，客户端与服务端之间虽然可以独立开发，但还是存在耦合。比如，客户端在发送请求的时，必须知道服务器的地址，且必须保证服务器正常工作

基于上面这几个原因，所以OpenStack才采用了另一种远程通信机制，这就是我们今天要讨论的鼎鼎大名的RPC。

今天的话题，就是源码解读OpenStack是如何通过rpc进行远程调用的。

如果你不想用现成的 `notification_event_types`，而想新定义一个，可以这样做

首先在这里先定义合法的 `notification_event_types`，相当于添加白名单。

![](http://image.python-online.cn/20190526172514.png)

然后在调用处，使用 `rpc.get_notifier` 来发送消息给ceilometer。

![](http://image.python-online.cn/20190526172725.png)

继续查看 `rpc.get_notifier` 做了什么事？如何实现直接info 就能发送消息的。

![](http://image.python-online.cn/20190526173314.png)

当你使用的event_types 不在白名单内，或者是异常信息。就会给打印warn日志

![](http://image.python-online.cn/20190526175100.png)



在nova.confg里有这一项配置，rabbit 指明了通信方式是rabbitmq。后面我标名了user，passwd，主机域名，端口，你应该能理解。

```
transport_url=rabbit://user:passwd@ctrl.openstack.com:5672
```

如果有多个主机，可以参照下面的 docstring 使用 逗号分隔进行添加。

![](http://image.python-online.cn/20190526182125.png)

指明了使用 messagingv2 这个driver，其他可用的还有routing, log, test, noop

```
[oslo_messaging_notifications]
driver = messagingv2
```

在rabbit里查看队列，notification 是 topic

![](http://image.python-online.cn/20190526180708.png)

而 debug ，info 等是event priority

![](http://image.python-online.cn/20190526181433.png)

当nova-api发送rpc消息给conducotr的时候，是如何发送过去的呢？

这边是 nova/conducotr/rpcapi.py，是发送端。

target 里的 namespace 指定了要调用哪个类。

cctxt.call 里的 'build_instances' 指定了要调用哪个函数。

![](http://image.python-online.cn/20190526185217.png)

在nova/conducotr/manager.py 里有两个类，各有一个类变量，声明了target，发送端的target需要和这边target匹配上，才会进入相应的处理逻辑。

![](http://image.python-online.cn/20190526184854.png)

你一定很好奇，这个target是如何绑定到rpc server里的呢？

首先我们要知道 rpc_server 是如何创建的？

如果你看了 8.5.11 那节 （nova的各项服务是怎么启动？），然后再去看 nova-conductor的启动代码，你就会发现在 nova\nova\service.py:Service.start() 里有这么一段代码，是创建rpc server的，endpoints 里面的self.manager 就是ConductorManager 类

![](http://image.python-online.cn/20190526221219.png)

![](http://image.python-online.cn/20190526221636.png)

从oslo_messaging 的代码可以看出，它会先根据endpoints创建dispatcher，dispatcher就会根据类变量target对象，创建消息的分发规则。这样从client就可以成功地调用远程（服务端）的函数。

![](http://image.python-online.cn/20190526220809.png)

![](http://image.python-online.cn/20190526220605.png)

参考文章：[OpenStack之RPC调用（一）](https://blog.csdn.net/qiuhan0314/article/details/42671965)



## 8.5.3 创建快照代码解读？



## 8.5.4 虚拟机状态

vm_state 描述虚拟机当前的稳定状态，其值可以在 `nova/compute/vm_states.py`看到

```
ACTIVE
BUILDING
PAUSED
SUSPENDED
STOPPED
RESCUED
RESIZED
SOFT_DELETED
DELETED
ERROR
SHELVED
```

power_state 描述的是从hypervisor传过来的状态，其值可在`nova/compute/power_state.py`

```
NOSTATE
RUNNING
PAUSED
SHUTDOWN
CRASHED
SUSPENDED
```

task_state 描述的是中间任务状态，

```
spawning
networking
scheduling
block_device_mapping
```

在创建虚拟机时，会有几次会产生虚拟机状态（vm_state和task_state）的上报（到ceilomet er）。

nova 提供了一个配置项：notify_on_state_change，本意是想，如果配置`vm_state`就只在vm_state

第一次，在`manager.py:2050`的函数 `_do_build_and_run_instance`里，看instance.save()



## 8.5.5 快照镜像如何实现？

nova-api 的入口如下

![](http://image.python-online.cn/20190508110723.png)

接着会调用 nova/compute/api.py

![](http://image.python-online.cn/20190508111109.png)

在nova-compute 层面：nova/compute/manager.py:_snapshot_instance()

![](http://image.python-online.cn/20190508095028.png)

接下来会调用 `nova/virt/libvirt/driver.py:snapshot()`

![](http://image.python-online.cn/20190508111527.png)

先获取imagebackend的类型，然后找到对应的backend

```python
disk_path, source_format = libvirt_utils.find_disk(virt_dom)
source_type = libvirt_utils.get_disk_type_from_path(disk_path)
...
snapshot_backend = self.image_backend.snapshot(instance,
                                               disk_path,
                                               image_type=source_type)
```

接下来，会调用对应的imagebackend的`snapshot_extract` 方法。

![](http://image.python-online.cn/FhRPy4B1xEI9SfoD2RcunJl15ZE3)

`snapshot_extract` 方法最终会调用`nova/virt/images.py:_convert_image()` ，可以看出其实底层调用的是 `qemu-img` 提供的`convert` 接口。

![](http://image.python-online.cn/FuyMWZS6HF4g3rPwTlLcereZxg4L)

如果是qcow2的backend，不是调用这边，而是调用 `nova/virt/libvirt/utils.py:extract_snapshot()`

![1551944122412](C:\Users\wangbm\AppData\Roaming\Typora\typora-user-images\1551944122412.png)

如果要查询镜像压缩的时间，可以在compute上执行这个命令

```
grep -E "Start executing commands|End executing commands" /var/log/nova/nova-compute.log
```

以上，就是整个镜像创建的过程。

独立磁盘模式的暂时不支持，原因分析如下。

在`libvirt_utils.get_disk_type_from_path` 里没有相应的修改，导致返回的是lvm。

![](http://image.python-online.cn/FnJA8RNIvJN2lAEXbKtJDpOLg1vg)

后面的imagebackend也相应的变成 lvm的

![](http://image.python-online.cn/FnGyI8jCQFLCGi0pGVmI3SV6pDrv)

然后会进入 lvm这个backend的init函数。由于`path` 是`/dev/sdb` 并不是一个lv，所以这边会报错。

![1551940635806](C:\Users\wangbm\AppData\Roaming\Typora\typora-user-images\1551940635806.png)

下次修改方法：一个是最开始获取`source_type`时判断为isolate，一个是后面 `isolate`的`extract_snapshot` 也要和lvm一样实现一下。



## 8.5.6 宿主机资源采集上报

compute的资源上报，是在 `nova/compute/resource_tracker.py:_init_compute_node` 里。

从宿主机上获取数据：`update_available_resource` 函数下的 `resources = self.driver.get_available_resource(self.nodename)` 其调用的函数是`virt/libvirt/driver.py` 里的 `get_available_resource` 函数

![](http://image.python-online.cn/FrbE6oEZ3vtTWwDfMNQ16MGi6SWr)


从数据库获取旧数据 `self.compute_node = self._get_compute_node(context)` 

## 8.5.7 资源主机调度实现

一般情况下一个 OpenStack 中，会部署有许多个计算节点。当我们创建一个虚拟机时，OpenStack 如何决定要将我们的虚拟机创建在哪里呢？这就是 openstack-nova-scheduler 要做的事，顾名思义，它是对集群内的所有计算节点的资源情况进行比较，从而选出一台最适合我们当前虚拟机创建的节点，再把请求发到 这一台节点上的 openstack-nova-compute 去进行真正的创建过程。

从源代码中看，最开始是 nova-conductor （nova/conductor/manager.py）在给 nova-compute 发创建请求前，会先让 nova-scheduler 选出一台资源充足的计算节点。

![](http://image.python-online.cn/20190424212211.png)

nova-scheduler 的调度主要由两部分组成

![](http://image.python-online.cn/20190424213430.png)

- 过滤器：filter，将不满足条件（硬性条件，比如内存，cpu，磁盘，pci设备等）的计算节点，直接过滤掉。意义：从过滤器出来的那些计算节点，理论上都可以创建虚拟机。
- 称重器：weigher，对满足硬性条件的众多主机按照一定的规则进行权重配比。意义：经过称重器计算，选出你更希望在哪台节点上创建虚拟机。

不管是过滤器，还是称重器，它们都需要两个参数

- hosts：多个 host_state 的集合，包含有当前可用的计算节点信息（资源，ip等）。其中单个元素是 HostState （nova/scheduler/host_manager.py）类的实例。如果你想添加其他原来没有的信息，比如 compute 的 id，可以在 `_update_from_compute_node`  函数中添加。它会从compute_nodes 表中取得你想要的信息。

  ![](http://image.python-online.cn/20190424214653.png)

- spec_obj：你所要请求创建的虚拟机信息（模板，镜像等）。它是从 objects.RequestSpec.from_primitives 中取得的

  ![](http://image.python-online.cn/20190424214540.png)


过滤器，它的代码如下：

![](http://image.python-online.cn/20190424221602.png)

称重器，它的规则主要看这段代码。

![](http://image.python-online.cn/20190424215735.png)

我在代码中，加了几段日志。从左到右，三个不同颜色的内容分别为，原始权值，配重系数（越高说明越占比越大，越影响最终结果），经过 nomalize 后的权值（只有 0 和 1，我觉得原代码这块应该要有浮点数）。

![](http://image.python-online.cn/20190424220008.png)

那最终的权值如何计算呢？

1. 先计算每一个称重器后的权重： weights * multipier
2. 最后按不同的compute 将权重相加起来。



## 8.5.8 手动引入上下文环境

有两种方式可以生成context

1. 如果有请求req（在nova-api里），可以使用这种

![](http://image.python-online.cn/20190426153322.png)

2. 其他地方可以使用这种

![](http://image.python-online.cn/20190426152148.png)

## 8.5.9 指定ip时检查allocation_pools

在原生的 neutron 中，当你指定 ip（172.20.22.64） 来创建虚拟机时，假如子网的 allocation_pools 是 172.20.20.100 - 172.20.20.200 ，那 neutron 是不会去检查你指定的ip是否在 allocation_pools 中的。

先来看看，port 是如何创建的

![](http://image.python-online.cn/20190526141815.png)



若要解决这个问题，可以参考原生代码中，在为子网添加allocation_pool时，验证是否合法的的逻辑，代码如下

![](http://image.python-online.cn/20190526142453.png)

然后在 `neutron\neutron\db\ipam_pluggable_backend.py` 文件中添加我们检查 ip是否在 allocation_pools 中的逻辑代码。

![](http://image.python-online.cn/20190526134519.png)

```python
    # 代码如下：方便复制
    @staticmethod
    def _is_ip_in_allocation_pools(ip_address, allocation_pools):
        from neutron.ipam.exceptions import InvalidIpForAllocationPools

        for ap in allocation_pools:
            ap_start_ip = netaddr.IPAddress(ap['start'])
            ap_end_ip = netaddr.IPAddress(ap['end'])
            if ap_start_ip <= ip_address <= ap_end_ip:
                return True
        raise InvalidIpForAllocationPools(ip_address='ip_address')

    def _validate_allocation_pool_for_fixed_ip(self, subnet, fixed):
        ip_address = netaddr.IPAddress(fixed["ip_address"])
        allocation_pools = subnet["allocation_pools"]
        return self._is_ip_in_allocation_pools(ip_address, allocation_pools)
```

然后还要定义一个异常类型

![](http://image.python-online.cn/20190526141226.png)

若指定的ip在allocation pool 里，则正常创建，若不在allocation 里，就会在 nova-compute 日志中报错。

![](http://image.python-online.cn/20190526134543.png)

可以发现我们的ip 172.20.22.64 并不在子网的allocation pool，理所当然在nova的日志中可以看到相应的报错。

![](http://image.python-online.cn/20190526134618.png)

## 8.5.10 attach port时ip占用提示

当你调用 `os-interface` （指定了ip）接口给一台虚拟机添加一张网卡时，若这个ip已经被使用。

nova-api 返回的结果令人无法理解：

```
 [{"computeFault": {"message": "Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n<class 'oslo_messaging.rpc.client.RemoteError'>", "code": 500}}]. 
```

究其原因，是 nova 在调用neutron的api 创建port时，如果ip已被占用，必须neutron会抛出 IpAddressAlreadyAllocated，而在 neutronclient 只有 IpAddressInUseClient 的异常，并不匹配，在neutronclient 端与neutron 对应的异常应该为 IpAddressAlreadyAllocatedClient 。

![](http://image.python-online.cn/20190526140213.png)

如何让nova-api能够返回具体的错误信息呢？

解决方法有两种，

一种是，在 neutronclient/common/exceptions.py 里添加 IpAddressAlreadyAllocatedClient 异常。

并且在nova 创建port的代码处，捕获这个异常

![](http://image.python-online.cn/20190526140301.png)

这种要改两个组件，而且要将neutronclient 的代码也管理起来，较为麻烦

一种是，只改neutron，在neutron/ipam/exceptions.py 添加一个与 neutronclient 相对应的异常。

![](http://image.python-online.cn/20190526140315.png)

然后修改 neutron/ipam/drivers/neutrondb_ipam/drivers.py 修改异常类型

![](http://image.python-online.cn/20190526140336.png)

通过 postman 进行模拟，已经可以返回具体的信息

![](http://image.python-online.cn/20190526140410.png)

另附：neutron 是如何判断ip是否已经占用？代码如下

![](http://image.python-online.cn/20190526143235.png)

## 8.5.11 nova的各项服务服务是如何启动的？

nova 里有不少服务，比如 nova-compute，nova-api，nova-conductor，nova-scheduler 等。

这些服务如何都是如何启动的呢？他们其实都是用同一套代码，所以只要分析一个就行，这里以nova-compute为例来了解一下。

从 /usr/bin/nova-compute 这个文件可以了解到nova-compute的入口是 `nova.cmd.compute:main()`

![](http://image.python-online.cn/20190526205152.png)

从这个入口进去，会开启一个 `nova-compute` 的服务。

![](http://image.python-online.cn/20190526165007.png)

当调用 service.Service.create 时，实际是返回实际化 service.Service 对象。当没有传入 manager 时，就以binary 里的为准。比如binary 是` nova-compute`，那manager_cls 就是 `compute_manager`，对应的manager 导入路径，就会从配置里读取。

![](http://image.python-online.cn/20190526204328.png)





---

![关注公众号，获取最新干货！](http://image.python-online.cn/20190511161447.png)